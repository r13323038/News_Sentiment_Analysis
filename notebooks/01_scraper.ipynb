{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0eb330",
   "metadata": {},
   "source": [
    "# Part 0 Scarper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d9334",
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `pip` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `pip` not defined in `Main`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ c:\\Users\\a5217\\Sync\\fiisual\\notebooks\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W0sZmlsZQ==.jl:1"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import GoogleNews\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import sys  # å¼•å…¥ sys ä»¥ä¾¿å¼·åˆ¶ä¸­æ­¢ç¨‹å¼\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# drive_folder = '/content/drive/MyDrive/fiisual/news_data2'\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)\n",
    "file_path = os.path.join(project_root, 'data', 'archive', 'all_data.csv')\n",
    "\n",
    "drive_folder = os.path.join(base_dir, 'fiisual', 'news_data2')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================= ğŸ“ å­˜æª”èˆ‡é€²åº¦ç®¡ç† =================\n",
    "\n",
    "\n",
    "if not os.path.exists(drive_folder):\n",
    "    os.makedirs(drive_folder)\n",
    "\n",
    "def get_completed_dates(folder_path):\n",
    "    \"\"\"æƒæè³‡æ–™å¤¾ï¼Œæ‰¾å‡ºå·²ç¶“çˆ¬å®Œçš„æ—¥æœŸ (æ ¼å¼ YYYYMMDD)\"\"\"\n",
    "    files = glob.glob(os.path.join(folder_path, 'yahoo_news_*.csv'))\n",
    "    completed = set()\n",
    "    for f in files:\n",
    "        filename = os.path.basename(f)\n",
    "        try:\n",
    "            date_part = filename.split('_')[-1].replace('.csv', '')\n",
    "            completed.add(date_part)\n",
    "        except:\n",
    "            pass\n",
    "    return completed\n",
    "\n",
    "# ================= çˆ¬èŸ²è¨­å®š =================\n",
    "days_back = 30\n",
    "target_results_per_day = 100\n",
    "pages_to_scrape = target_results_per_day // 10\n",
    "\n",
    "# åŸºç¤é—œéµå­—\n",
    "base_query = ('site:finance.yahoo.com/news '\n",
    "        '(\"Bloomberg\" OR \"Reuters\" OR \"CNBC\" ) '\n",
    "        '-site:sg.finance.yahoo.com -site:ca.finance.yahoo.com -site:nz.finance.yahoo.com -site:uk.finance.yahoo.com -site:au.finance.yahoo.com '\n",
    "        '-\"The Motley Fool\" -Zacks -TipRanks -\"GuruFocus.\" '\n",
    "        '-\"Insider Monkey\" -\"InvestorPlace\" -\"Benzinga\" -\"Seeking Alpha\" '\n",
    "        '-\"Press Release\" -GlobeNewswire -\"Business Wire\" -\"PR Newswire\" -\"Accesswire\"'\n",
    "        )\n",
    "\n",
    "# ==========================================\n",
    "\n",
    "# è¨­å®šåŸºæº–æ—¥\n",
    "end_date = datetime(2025, 12, 14)\n",
    "date_list = [end_date - timedelta(days=x) for x in range(days_back)]\n",
    "\n",
    "# ğŸ” è®€å–å·²å®Œæˆçš„é€²åº¦\n",
    "completed_dates = get_completed_dates(drive_folder)\n",
    "print(f\"ğŸ“‚ ç›®å‰å·²å­˜æª” {len(completed_dates)} å¤©çš„è³‡æ–™ï¼Œå°‡è‡ªå‹•è·³éé€™äº›æ—¥æœŸã€‚\")\n",
    "print(f\"ğŸ¢ å•Ÿå‹• GoogleNews (åµæ¸¬ 429 è‡ªå‹•åœæ­¢ç‰ˆ)... å­˜æª”è‡³ï¼š{drive_folder}\\n\")\n",
    "\n",
    "for target_date in date_list:\n",
    "    file_date_str = target_date.strftime('%Y%m%d')\n",
    "    date_str = target_date.strftime('%m/%d/%Y')\n",
    "\n",
    "    # === ğŸ›‘ æª¢æŸ¥é»ï¼šå¦‚æœé€™å¤©è·‘éäº†ï¼Œå°±è·³é ===\n",
    "    if file_date_str in completed_dates:\n",
    "        print(f\"â­ï¸ [{date_str}] æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éã€‚\")\n",
    "        continue\n",
    "\n",
    "    # è¨ˆç®—æ—¥æœŸå€é–“\n",
    "    day_before = target_date - timedelta(days=1)\n",
    "    day_after = target_date + timedelta(days=1)\n",
    "    str_after = day_before.strftime('%Y-%m-%d')\n",
    "    str_before = day_after.strftime('%Y-%m-%d')\n",
    "    standard_date_str = target_date.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"ğŸ“… [{date_str}] è™•ç†ä¸­... (ç›®æ¨™: {target_results_per_day} ç¯‡)\")\n",
    "\n",
    "    googlenews = GoogleNews(lang='en', region='US')\n",
    "    final_query = f'{base_query} after:{str_after} before:{str_before}'\n",
    "\n",
    "    # å˜—è©¦åˆå§‹åŒ–æœå°‹\n",
    "    try:\n",
    "        googlenews.search(final_query)\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"  åˆå§‹åŒ–å¤±æ•— \")\n",
    "            continue\n",
    "\n",
    "    # === çˆ¬å–é é¢ ===\n",
    "    daily_results = []\n",
    "\n",
    "    for page in range(1, pages_to_scrape):\n",
    "        try:\n",
    "            googlenews.get_page(page)\n",
    "\n",
    "            # éš¨æ©Ÿå»¶é² (ç¨å¾®æ‹‰é•·ä¸€é»é»ï¼Œé™ä½è¢«å°é–æ©Ÿç‡)\n",
    "            sleep_time = random.uniform(1, 3)\n",
    "            print(f\"   -> Page {page} done. ä¼‘æ¯ {int(sleep_time)}s...\", end='\\r')\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "\n",
    "            # === ğŸš¨ é—œéµä¿®æ”¹ï¼šåµæ¸¬ 429 é€™è£¡æœ€é‡è¦ ===\n",
    "            if \"429\" in error_msg or \"Too Many Requests\" in error_msg:\n",
    "                print(\"\\n\" + \"=\"*40)\n",
    "                print(f\"ğŸ›‘ åµæ¸¬åˆ° 429 å°é–ï¼(æ–¼ç¬¬ {page} é )\")\n",
    "                print(f\"ğŸš¨ è«‹åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š\")\n",
    "                print(f\"   1. é¸å–®ã€åŸ·è¡Œéšæ®µã€‘->ã€ä¸­æ–·é€£ç·šä¸¦åˆªé™¤åŸ·è¡Œéšæ®µã€‘\")\n",
    "                print(f\"   2. é‡æ–°é€£ç·šä¸¦åŸ·è¡Œç¨‹å¼ï¼Œå°‡å¾ [{date_str}] ç¹¼çºŒã€‚\")\n",
    "                print(\"=\"*40 + \"\\n\")\n",
    "                sys.exit(\"IP Blocked - Stop Execution\") # å¼·åˆ¶åœæ­¢\n",
    "            else:\n",
    "                print(f\"\\n   âš ï¸ Page {page} ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e} (å˜—è©¦ç¹¼çºŒ)\")\n",
    "\n",
    "    # === å­˜æª”å€å¡Š ===\n",
    "    daily_data = googlenews.result()\n",
    "\n",
    "    if daily_data:\n",
    "        df = pd.DataFrame(daily_data)\n",
    "        if 'title' in df.columns:\n",
    "            # 1. ç§»é™¤ç©ºæ¨™é¡Œèˆ‡é‡è¤‡é€£çµ\n",
    "            df = df[df['title'].astype(bool)]\n",
    "            df = df.drop_duplicates(subset=['link'])\n",
    "\n",
    "            df['date'] = standard_date_str\n",
    "            # ====================================================\n",
    "\n",
    "            # å­˜æª”\n",
    "            cols_to_keep = ['date', 'title', 'media', 'link'] # æŠŠ date æ”¾ç¬¬ä¸€æ¬„æ–¹ä¾¿çœ‹\n",
    "            df = df[cols_to_keep] # åªç•™é€™äº›æ¬„ä½ï¼Œå…¶ä»–é›œäº‚çš„éƒ½ä¸Ÿæ‰\n",
    "\n",
    "            filename = f'yahoo_news_{target_date.strftime(\"%Y%m%d\")}.csv'\n",
    "            full_path = os.path.join(drive_folder, filename)\n",
    "\n",
    "            df.to_csv(full_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\nâœ… [{standard_date_str}] å­˜æª”å®Œæˆ ({len(df)} ç­†)\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ [{date_str}] ç„¡è³‡æ–™ã€‚\")\n",
    "\n",
    "    print(\"â˜• æ›æ—¥ä¼‘æ¯ 10 ç§’...\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"ğŸ‰ å…¨éƒ¨ä»»å‹™å®Œæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".finbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
